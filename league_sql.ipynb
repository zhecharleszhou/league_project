{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://leportella.com/english/2019/01/10/sqlalchemy-basics-tutorial.html?fbclid=IwAR15Ck3iit3b1kfd4iE3ZhNtEHbvs8mP7gHgAaguy0Ts9VNBD7AurRKn3zM\n",
    "import sqlalchemy\n",
    "import single_user_blitz_grabber\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "## Parameters\n",
    "APIKey = os.environ.get('League_API')\n",
    "region = 'na1'\n",
    "summoner_name = 'Duvet Cover'\n",
    "table_name = 'matches_2021'\n",
    "flag_make_new_table = False\n",
    "flag_add_new_matches = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# connects to mysql server table\n",
    "engine = sqlalchemy.create_engine('mysql://root:Ironmaiden1!@localhost/duvet_cover_matches') # connect to server\n",
    "engine.connect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recreates an empty table\n",
    "if flag_make_new_table:\n",
    "    if engine.dialect.has_table(engine, table_name):\n",
    "        \n",
    "        with engine.connect() as connection:\n",
    "            result = connection.execute(f\"\"\"DROP TABLE {table_name}\"\"\")\n",
    "        \n",
    "    with engine.connect() as connection:\n",
    "        result = connection.execute(f\"\"\"CREATE TABLE {table_name} (acct_id VARCHAR(50), match_id BIGINT(10) PRIMARY KEY, match_rank VARCHAR(10), \n",
    "                                    role VARCHAR(20), champ VARCHAR(10), win FLOAT(2), kills FLOAT(3), deaths FLOAT(3),\n",
    "                                    assists FLOAT(3), gold_earned FLOAT(20), vision_score INT(4), crowd_control_time INT(4), dmg_taken INT(6), \n",
    "                                    dmg_dealt INT(6), objective_dmg INT(6), player_top VARCHAR(20), player_jung VARCHAR(20),\n",
    "                                    player_mid VARCHAR(20), player_ADC VARCHAR(20), player_supp VARCHAR(20), opp_top VARCHAR(20),\n",
    "                                    opp_jung VARCHAR(20), opp_mid VARCHAR(20), opp_ADC VARCHAR(20), opp_supp VARCHAR(20))\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if flag_add_new_matches:\n",
    "    # grabs recent matches for the user\n",
    "    df, timeline_data = single_user_blitz_grabber.main_grab_data(region,summoner_name, APIKey)\n",
    "    \n",
    "    # code below either sends df to the sql table or append to the existing data\n",
    "    if flag_make_new_table:\n",
    "        # send dataframe to sql table\n",
    "        df.to_sql(con=engine, name=table_name, if_exists='append', index=False)\n",
    "    # append df to existing data table; roundabout way to do this is to make a temp sql table and merge it using sql commands \n",
    "    else:\n",
    "        df.to_sql(name='temporary_table', con=engine, if_exists = 'replace', index=False)\n",
    "\n",
    "        with engine.begin() as temp_to_target_table:\n",
    "            insert_sql = f'INSERT IGNORE INTO {table_name} (SELECT * FROM temporary_table)'\n",
    "            temp_to_target_table.execute(insert_sql)\n",
    "\n",
    "        with engine.connect() as connection:\n",
    "                result = connection.execute(f\"\"\"DROP TABLE temporary_table\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data viz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some viz inspired by https://www.kaggle.com/roshansharma/breast-cancer-wisconsin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import seaborn as sns # data visualization library  \n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_sql(f'SELECT * FROM {table_name}', con=engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"make the column names reference-friendly (eg. replace spaces with underscore, make everything lowercase, etc. \n",
    "they already are, but just to be safe!\"\"\"\n",
    "\n",
    "df.columns = df.columns.str.strip().str.lower().str.replace(' ', '_')\n",
    "\n",
    "col = df.columns       # .columns gives columns names in data \n",
    "print(f'num matches: {df.shape[0]}')\n",
    "print(col)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['kda'] = (df['kills']+df['assists'])/df['deaths'].replace(0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "\n",
    "df_numerical = df.select_dtypes(include=numerics).drop(columns=[ 'match_id'])\n",
    "df_numerical.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic viz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn win/loss to boolean\n",
    "y_bool = df['win']!= 0\n",
    "x = df_numerical.drop(columns='win')\n",
    "\n",
    "ax = sns.countplot(y_bool,label=\"Count\")       # M = 212, B = 357\n",
    "num_win, num_loss = y_bool.value_counts()\n",
    "print('Number of win: ',num_win)\n",
    "print('Number of loss : ',num_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data_dia = y_bool\n",
    "data = x\n",
    "data_n_2 = (data - data.mean()) / (data.std())              # standardization\n",
    "data = pd.concat([y_bool,data_n_2.iloc[:,0:10]],axis=1)\n",
    "data = pd.melt(data,id_vars=\"win\",\n",
    "                    var_name=\"features\",\n",
    "                    value_name='value')\n",
    "plt.figure(figsize=(10,7))\n",
    "sns.violinplot(x=\"features\", y=\"value\", hue=\"win\", data=data,split=True, inner=\"quart\")\n",
    "plt.xticks(rotation=45);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style=\"white\")\n",
    "df = x.loc[:,['gold_earned','kda','vision_score','dmg_taken', 'dmg_dealt','objective_dmg']]\n",
    "g = sns.PairGrid(df, diag_sharey=False)\n",
    "g.map_lower(sns.kdeplot, cmap=\"Blues_d\")\n",
    "g.map_upper(plt.scatter)\n",
    "g.map_diag(sns.kdeplot, lw=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression model to identify key variables to win/loss outcome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import GridSearchCV,train_test_split,cross_val_score\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate outcome (dependent variable) and feature data (independent variables)\n",
    "dataX=df_numerical.drop('win',axis=1)\n",
    "dataY=df_numerical['win']\n",
    "\n",
    "num_features = len(dataX.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our variables span very different scales; to make sure that the large or small scale of variables does not artificially affect how much they affect the model, we will normalize each variable's scale from 0 to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate encoder/scaler\n",
    "scaler = StandardScaler()\n",
    "dataX_scaled  = scaler.fit_transform(dataX) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from scikitlearn: split data into test and training sets\n",
    "xTrain,xTest,yTrain,yTest=train_test_split(dataX,dataY,test_size=0.2,random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build the Logistic regression\n",
    "\n",
    "GridSearchCV allows us to define a set of parameters to test. The function iteratively evaluates each parameter combination and will choose the best model parameters.\n",
    "\n",
    "Smaller values of C specify stronger regularization (penalty for overfitting)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters=dict(penalty=['l2'],\n",
    "    C = np.logspace(-4,4,15),\n",
    "    random_state=[0],)\n",
    "\n",
    "logOptimal = GridSearchCV(LogisticRegression(), parameters, scoring='accuracy')\n",
    "logOptimal.fit(xTrain, yTrain)\n",
    "print('Best parameters set:')\n",
    "log_opt_params = logOptimal.best_params_\n",
    "print(log_opt_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get predictions from our model and assess the accuracy\n",
    "\n",
    "pred = logOptimal.predict(xTest)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "print('Optimized logistic regression performance: ',\n",
    "      round(accuracy_score(yTest,pred),5)*100,'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Examine contribution of variables to win\n",
    "\n",
    "We can pull out coefficients for each independent variable. Because we scaled all the independent variables in a previous step, we can compare the relative values of the coefficients. Higher coefficients indicate stronger influence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the best parameters from the last step\n",
    "bestLR=LogisticRegression(C=log_opt_params['C'],\n",
    "                          penalty=log_opt_params['penalty'],\n",
    "                          random_state=log_opt_params['random_state'])\n",
    "bestLR.fit(xTrain, yTrain)\n",
    "\n",
    "df_log_reg_coeffs = bestLR.coef_[0]\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.bar(np.arange(num_features), df_log_reg_coeffs)\n",
    "plt.ylabel('Coef Score')\n",
    "plt.xticks(np.arange(num_features), dataX.columns, rotation=45)\n",
    "plt.title('Log Reg Coef Scores')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline to "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a pipeline to search for the best combination of PCA truncation\n",
    "# and classifier regularization.\n",
    "std_scaler = StandardScaler()\n",
    "pca = PCA()\n",
    "# set the tolerance to a large value to make the example faster\n",
    "logistic = LogisticRegression(max_iter=10000, tol=0.1)\n",
    "\n",
    "# Parameters of pipelines can be set using ‘__’ separated parameter names:\n",
    "param_grid = {\n",
    "    'pca__n_components': np.arange(1,5),\n",
    "    'logistic__C': np.logspace(-4,4,15),\n",
    "}\n",
    "\n",
    "log_reg_pipe = Pipeline(steps=[('standard_scaler', std_scaler),\n",
    "                               ('pca', pca), \n",
    "                               ('logistic', logistic)])\n",
    "\n",
    "GS_log_reg = GridSearchCV(log_reg_pipe,\n",
    "                          param_grid, \n",
    "                          cv=5, \n",
    "                          n_jobs=1, \n",
    "                          scoring = 'neg_mean_squared_error')\n",
    "\n",
    "GS_log_reg.fit(xTrain, yTrain)\n",
    "\n",
    "print(\"Best parameter (CV score=%0.3f):\" % GS_log_reg.best_score_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GS_log_reg.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the PCA spectrum\n",
    "pca.fit(xTest)\n",
    "\n",
    "fig, (ax0, ax1) = plt.subplots(nrows=2, sharex=True, figsize=(6, 6))\n",
    "ax0.plot(np.arange(1, pca.n_components_ + 1),\n",
    "         pca.explained_variance_ratio_, '+', linewidth=2)\n",
    "ax0.set_ylabel('PCA explained variance ratio')\n",
    "\n",
    "ax0.axvline(GS_log_reg.best_estimator_.named_steps['pca'].n_components,\n",
    "            linestyle=':', label='n_components chosen')\n",
    "ax0.legend(prop=dict(size=12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
